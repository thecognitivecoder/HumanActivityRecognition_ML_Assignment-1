{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7dd5d51-d204-45f3-aeff-20136dc646d5",
   "metadata": {},
   "source": [
    "### Question 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "93fac09a-4612-4fea-af72-a22ccf29c77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:25: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  total_acc_x = pd.read_csv(os.path.join(train_path,\"Inertial Signals\",\"total_acc_x_train.txt\"),delim_whitespace=True,header=None)\n",
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:26: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  total_acc_y = pd.read_csv(os.path.join(train_path,\"Inertial Signals\",\"total_acc_y_train.txt\"),delim_whitespace=True,header=None)\n",
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:27: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  total_acc_z = pd.read_csv(os.path.join(train_path,\"Inertial Signals\",\"total_acc_z_train.txt\"),delim_whitespace=True,header=None)\n",
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:31: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  subject_train = pd.read_csv(os.path.join(train_path,\"subject_train.txt\"),delim_whitespace=True,header=None)\n",
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:34: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y = pd.read_csv(os.path.join(train_path,\"y_train.txt\"),delim_whitespace=True,header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Combining the training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:80: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  total_acc_x = pd.read_csv(os.path.join(test_path,\"Inertial Signals\",\"total_acc_x_test.txt\"),delim_whitespace=True,header=None)\n",
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:81: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  total_acc_y = pd.read_csv(os.path.join(test_path,\"Inertial Signals\",\"total_acc_y_test.txt\"),delim_whitespace=True,header=None)\n",
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:82: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  total_acc_z = pd.read_csv(os.path.join(test_path,\"Inertial Signals\",\"total_acc_z_test.txt\"),delim_whitespace=True,header=None)\n",
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:85: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  subject_test = pd.read_csv(os.path.join(test_path,\"subject_test.txt\"),delim_whitespace=True,header=None)\n",
      "C:\\Users\\katta\\AppData\\Local\\Temp\\ipykernel_19172\\535492507.py:88: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  y = pd.read_csv(os.path.join(test_path,\"y_test.txt\"),delim_whitespace=True,header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done Combining the testing data\n",
      "Done Combining the data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn\n",
    "# Give the path of the test and train folder of UCI HAR Dataset\n",
    "train_path = r\"C:\\Users\\katta\\Downloads\\human+activity+recognition+using+smartphones\\UCI HAR Dataset\\UCI HAR Dataset\\train\"\n",
    "\n",
    "test_path = r\"C:\\Users\\katta\\Downloads\\human+activity+recognition+using+smartphones\\UCI HAR Dataset\\UCI HAR Dataset\\test\"\n",
    "\n",
    "# Dictionary of activities. Provided by the dataset.\n",
    "ACTIVITIES = {\n",
    "    1: 'WALKING'            ,\n",
    "    2: 'WALKING_UPSTAIRS'   ,\n",
    "    3: 'WALKING_DOWNSTAIRS' ,\n",
    "    4: 'SITTING'            ,\n",
    "    5: 'STANDING'           ,\n",
    "    6: 'LAYING'             ,\n",
    "}\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                        # Combining Traing Data\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Load all the accelerometer data\n",
    "total_acc_x = pd.read_csv(os.path.join(train_path,\"Inertial Signals\",\"total_acc_x_train.txt\"),delim_whitespace=True,header=None)\n",
    "total_acc_y = pd.read_csv(os.path.join(train_path,\"Inertial Signals\",\"total_acc_y_train.txt\"),delim_whitespace=True,header=None)\n",
    "total_acc_z = pd.read_csv(os.path.join(train_path,\"Inertial Signals\",\"total_acc_z_train.txt\"),delim_whitespace=True,header=None)\n",
    "\n",
    "\n",
    "# Read the subject IDs\n",
    "subject_train = pd.read_csv(os.path.join(train_path,\"subject_train.txt\"),delim_whitespace=True,header=None)\n",
    "\n",
    "# Read the labels\n",
    "y = pd.read_csv(os.path.join(train_path,\"y_train.txt\"),delim_whitespace=True,header=None)\n",
    "\n",
    "\n",
    "# Toggle through all the subjects.\n",
    "for subject in np.unique(subject_train.values):\n",
    "\n",
    "    sub_idxs = np.where( subject_train.iloc[:,0] == subject )[0]\n",
    "    labels = y.loc[sub_idxs]\n",
    "\n",
    "    # Toggle through all the labels.\n",
    "    for label in np.unique(labels.values):\n",
    "\n",
    "        # make the folder directory if it does not exist\n",
    "        if not os.path.exists(os.path.join(\"Combined\",\"Train\",ACTIVITIES[label])):\n",
    "            os.makedirs(os.path.join(\"Combined\",\"Train\",ACTIVITIES[label]))\n",
    "\n",
    "        label_idxs = labels[labels.iloc[:,0] == label].index\n",
    "\n",
    "        accx = []\n",
    "        accy = []\n",
    "        accz = []\n",
    "\n",
    "        for idx in label_idxs:\n",
    "            if accx is not None:\n",
    "                accx = np.hstack((accx,total_acc_x.loc[idx][64:]))\n",
    "                accy = np.hstack((accy,total_acc_y.loc[idx][64:]))\n",
    "                accz = np.hstack((accz,total_acc_z.loc[idx][64:]))\n",
    "\n",
    "            else:\n",
    "                accx = total_acc_x.loc[idx]\n",
    "                accy = total_acc_y.loc[idx]\n",
    "                accz = total_acc_z.loc[idx]\n",
    "\n",
    "        # saving the data into csv file\n",
    "        data = pd.DataFrame({'accx':accx,'accy':accy,'accz':accz})\n",
    "        save_path = os.path.join(\"Combined\",\"Train\",ACTIVITIES[label],f\"Subject_{subject}.csv\")\n",
    "        data.to_csv(save_path,index=False)\n",
    "\n",
    "print(\"Done Combining the training data\")\n",
    "\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                        # Combining Test Data               \n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Load all the accelerometer data\n",
    "total_acc_x = pd.read_csv(os.path.join(test_path,\"Inertial Signals\",\"total_acc_x_test.txt\"),delim_whitespace=True,header=None)\n",
    "total_acc_y = pd.read_csv(os.path.join(test_path,\"Inertial Signals\",\"total_acc_y_test.txt\"),delim_whitespace=True,header=None)\n",
    "total_acc_z = pd.read_csv(os.path.join(test_path,\"Inertial Signals\",\"total_acc_z_test.txt\"),delim_whitespace=True,header=None)\n",
    "\n",
    "# Read the subject IDs\n",
    "subject_test = pd.read_csv(os.path.join(test_path,\"subject_test.txt\"),delim_whitespace=True,header=None)\n",
    "\n",
    "# Read the labels\n",
    "y = pd.read_csv(os.path.join(test_path,\"y_test.txt\"),delim_whitespace=True,header=None)\n",
    "\n",
    "# Toggle through all the subjects.\n",
    "for subject in np.unique(subject_test.values):\n",
    "    \n",
    "        sub_idxs = np.where( subject_test.iloc[:,0] == subject )[0]\n",
    "        labels = y.loc[sub_idxs]\n",
    "\n",
    "        # Toggle through all the labels.\n",
    "        for label in np.unique(labels.values):\n",
    "    \n",
    "            if not os.path.exists(os.path.join(\"Combined\",\"Test\",ACTIVITIES[label])):\n",
    "                os.makedirs(os.path.join(\"Combined\",\"Test\",ACTIVITIES[label]))\n",
    "    \n",
    "            label_idxs = labels[labels.iloc[:,0] == label].index\n",
    "    \n",
    "            accx = []\n",
    "            accy = []\n",
    "            accz = []\n",
    "            for idx in label_idxs:\n",
    "                if accx is not None:\n",
    "                    accx = np.hstack((accx,total_acc_x.loc[idx][64:]))\n",
    "                    accy = np.hstack((accy,total_acc_y.loc[idx][64:]))\n",
    "                    accz = np.hstack((accz,total_acc_z.loc[idx][64:]))\n",
    "    \n",
    "                else:\n",
    "                    accx = total_acc_x.loc[idx]\n",
    "                    accy = total_acc_y.loc[idx]\n",
    "                    accz = total_acc_z.loc[idx]\n",
    "    \n",
    "            # saving the data into csv file\n",
    "            data = pd.DataFrame({'accx':accx,'accy':accy,'accz':accz})\n",
    "            save_path = os.path.join(\"Combined\",\"Test\",ACTIVITIES[label],f\"Subject_{subject}.csv\")\n",
    "            data.to_csv(save_path,index=False)\n",
    "\n",
    "print(\"Done Combining the testing data\")\n",
    "print(\"Done Combining the data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a317570-621b-487e-857e-51a5fe684d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape:  (126, 500, 3)\n",
      "Testing data shape:  (54, 500, 3)\n"
     ]
    }
   ],
   "source": [
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# Library imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\",\"SITTING\",\"STANDING\",\"WALKING\",\"WALKING_DOWNSTAIRS\",\"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6}\n",
    "\n",
    "combined_dir = os.path.join(\"Combined\")\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                                # Train Dataset\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_train=[]\n",
    "y_train=[]\n",
    "dataset_dir = os.path.join(combined_dir,\"Train\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
    "        df = df[offset:offset+time*50]\n",
    "        X_train.append(df.values)\n",
    "        y_train.append(classes[folder])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                                # Test Dataset\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "X_test=[]\n",
    "y_test=[]\n",
    "dataset_dir = os.path.join(combined_dir,\"Test\")\n",
    "\n",
    "for folder in folders:\n",
    "    files = os.listdir(os.path.join(dataset_dir,folder))\n",
    "    for file in files:\n",
    "\n",
    "        df = pd.read_csv(os.path.join(dataset_dir,folder,file),sep=\",\",header=0)\n",
    "        df = df[offset:offset+time*50]\n",
    "        X_test.append(df.values)\n",
    "        y_test.append(classes[folder])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "                                                # Final Dataset\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
    "\n",
    "# USE THE BELOW GIVEN DATA FOR TRAINING and TESTING purposes\n",
    "\n",
    "# concatenate the training and testing data\n",
    "X = np.concatenate((X_train,X_test))\n",
    "y = np.concatenate((y_train,y_test))\n",
    "\n",
    "# split the data into training and testing sets. Change the seed value to obtain different random splits.\n",
    "seed = 4\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=seed,stratify=y)\n",
    "\n",
    "print(\"Training data shape: \",X_train.shape)\n",
    "print(\"Testing data shape: \",X_test.shape)\n",
    "\n",
    "#=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-==-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bfd7365-08f6-43ff-87eb-e7e99d209d73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\n\\n# Load your data\\ndata = pd.read_csv(\\'/content/1 k.csv\\')\\n\\n# Display the original number of samples and first few rows to understand the structure\\nprint(f\"Original number of samples: {len(data)}\")\\nprint(data.head())\\n\\n# Downsample the data from 400Hz to 50Hz (keep every 8th sample)\\ndownsampled_data = data.iloc[::8, :].reset_index(drop=True)\\n\\n# Display the number of samples after downsampling\\nprint(f\"Number of samples after downsampling: {len(downsampled_data)}\")\\n\\n# Assuming 10 seconds of data at 50Hz => 500 samples\\ntotal_samples = len(downsampled_data)\\nsamples_to_keep = 500  # 10 seconds at 50Hz\\ntrim_start = (total_samples - samples_to_keep) // 2\\ntrim_end = trim_start + samples_to_keep\\n\\n# Trim to the middle 10 seconds\\ntrimmed_data = downsampled_data.iloc[trim_start:trim_end]\\n\\n# Display the final trimmed data\\nprint(f\"Number of samples after trimming: {len(trimmed_data)}\")\\nprint(trimmed_data.head())\\n\\n# Save the final processed data to a new CSV file\\ntrimmed_data.to_csv(\\'p_1k.csv\\', index=False)\\n\\nprint(\"Processed data saved successfully.\")'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('/content/1 k.csv')\n",
    "\n",
    "# Display the original number of samples and first few rows to understand the structure\n",
    "print(f\"Original number of samples: {len(data)}\")\n",
    "print(data.head())\n",
    "\n",
    "# Downsample the data from 400Hz to 50Hz (keep every 8th sample)\n",
    "downsampled_data = data.iloc[::8, :].reset_index(drop=True)\n",
    "\n",
    "# Display the number of samples after downsampling\n",
    "print(f\"Number of samples after downsampling: {len(downsampled_data)}\")\n",
    "\n",
    "# Assuming 10 seconds of data at 50Hz => 500 samples\n",
    "total_samples = len(downsampled_data)\n",
    "samples_to_keep = 500  # 10 seconds at 50Hz\n",
    "trim_start = (total_samples - samples_to_keep) // 2\n",
    "trim_end = trim_start + samples_to_keep\n",
    "\n",
    "# Trim to the middle 10 seconds\n",
    "trimmed_data = downsampled_data.iloc[trim_start:trim_end]\n",
    "\n",
    "# Display the final trimmed data\n",
    "print(f\"Number of samples after trimming: {len(trimmed_data)}\")\n",
    "print(trimmed_data.head())\n",
    "\n",
    "# Save the final processed data to a new CSV file\n",
    "trimmed_data.to_csv('p_1k.csv', index=False)\n",
    "\n",
    "print(\"Processed data saved successfully.\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16418248-8977-49cb-b0b2-c91e02be1a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\n\\n# List of your collected CSV files and corresponding activities\\ncsv_files = [\\'p_1k.csv\\', \\'p_2k.csv\\', \\'p_3k.csv\\', \\'p_4k.csv\\', \\'p_5k.csv\\', \\'p_6k.csv\\',\\n             \\'p_1-k.csv\\', \\'p_2-k.csv\\', \\'p_3-k.csv\\', \\'p_4-k.csv\\', \\'p_5-k.csv\\', \\'p_6-k.csv\\',\\n             \\'p_1-r.csv\\', \\'p_2-r.csv\\', \\'p_3-r.csv\\', \\'p_4-r.csv\\', \\'p_5-r.csv\\', \\'p_6-r.csv\\']\\nactivities = [\\'walking\\', \\'standing\\', \\'sitting\\', \\'laying\\', \\'walking_upstairs\\', \\'walking_downstairs\\',\\n              \\'walking\\', \\'standing\\', \\'sitting\\', \\'laying\\', \\'walking_upstairs\\', \\'walking_downstairs\\',\\n              \\'walking\\', \\'standing\\', \\'sitting\\', \\'laying\\', \\'walking_upstairs\\', \\'walking_downstairs\\']\\n\\n# Create an empty list to store all the data\\nall_data = []\\n\\n# Iterate over each CSV file and corresponding activity\\nfor i, csv_file in enumerate(csv_files):\\n    # Load the data\\n    activity_data = pd.read_csv(csv_file)\\n    \\n    # Rename columns to match the expected format\\n    activity_data.columns = [\\'time\\', \\'acc_x\\', \\'acc_y\\', \\'acc_z\\', \\'total_acc\\']  # Assuming your data has these columns\\n    \\n    # Add the activity label to the DataFrame\\n    activity_data[\\'activity\\'] = activities[i]\\n    \\n    # Append the DataFrame to the list\\n    all_data.append(activity_data)\\n\\n# Concatenate all DataFrames in the list into a single DataFrame\\nfinal_data = pd.concat(all_data, axis=0)\\n\\n# Save the concatenated DataFrame to a new CSV file\\nfinal_data.to_csv(\\'formatted_data.csv\\', index=False)\\n\\nprint(\"Data has been successfully formatted and saved to \\'formatted_data.csv\\'\")'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import pandas as pd\n",
    "\n",
    "# List of your collected CSV files and corresponding activities\n",
    "csv_files = ['p_1k.csv', 'p_2k.csv', 'p_3k.csv', 'p_4k.csv', 'p_5k.csv', 'p_6k.csv',\n",
    "             'p_1-k.csv', 'p_2-k.csv', 'p_3-k.csv', 'p_4-k.csv', 'p_5-k.csv', 'p_6-k.csv',\n",
    "             'p_1-r.csv', 'p_2-r.csv', 'p_3-r.csv', 'p_4-r.csv', 'p_5-r.csv', 'p_6-r.csv']\n",
    "activities = ['walking', 'standing', 'sitting', 'laying', 'walking_upstairs', 'walking_downstairs',\n",
    "              'walking', 'standing', 'sitting', 'laying', 'walking_upstairs', 'walking_downstairs',\n",
    "              'walking', 'standing', 'sitting', 'laying', 'walking_upstairs', 'walking_downstairs']\n",
    "\n",
    "# Create an empty list to store all the data\n",
    "all_data = []\n",
    "\n",
    "# Iterate over each CSV file and corresponding activity\n",
    "for i, csv_file in enumerate(csv_files):\n",
    "    # Load the data\n",
    "    activity_data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Rename columns to match the expected format\n",
    "    activity_data.columns = ['time', 'acc_x', 'acc_y', 'acc_z', 'total_acc']  # Assuming your data has these columns\n",
    "    \n",
    "    # Add the activity label to the DataFrame\n",
    "    activity_data['activity'] = activities[i]\n",
    "    \n",
    "    # Append the DataFrame to the list\n",
    "    all_data.append(activity_data)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "final_data = pd.concat(all_data, axis=0)\n",
    "\n",
    "# Save the concatenated DataFrame to a new CSV file\n",
    "final_data.to_csv('formatted_data.csv', index=False)\n",
    "\n",
    "print(\"Data has been successfully formatted and saved to 'formatted_data.csv'\")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e92a8656-6177-40c8-be21-9f73890c215e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.61\n",
      "Precision: 0.56\n",
      "Recall: 0.61\n",
      "Confusion Matrix:\n",
      "[[2 2 2 3 0 0]\n",
      " [2 5 1 0 1 0]\n",
      " [4 2 1 1 0 1]\n",
      " [0 0 0 8 1 0]\n",
      " [0 1 0 0 8 0]\n",
      " [0 0 0 0 0 9]]\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "classifier.fit(X_train.reshape(X_train.shape[0], -1), y_train)  # Flatten for fitting\n",
    "y_pred = classifier.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "106f1b1b-b7bc-4488-bff8-890494caddbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mean_acc_x  mean_acc_y  mean_acc_z  std_acc_x  std_acc_y  std_acc_z  \\\n",
      "0    0.040473   -1.006204    0.125954   0.113420   0.089697   0.126751   \n",
      "1   -0.014663   -1.018911    0.038372   0.008216   0.004003   0.006457   \n",
      "2   -0.221789   -0.381687    0.901249   0.003381   0.003160   0.002411   \n",
      "3    0.434268   -0.015518    0.904393   0.007132   0.005794   0.003771   \n",
      "4   -0.491491   -0.837544    0.179951   0.164943   0.263060   0.262903   \n",
      "\n",
      "   mean_total_acc  std_total_acc  activity  \n",
      "0        1.029248       0.086687         1  \n",
      "1        1.019816       0.003932         5  \n",
      "2        1.003568       0.002340         4  \n",
      "3        1.003392       0.004891         6  \n",
      "4        1.039822       0.244000         2  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the collected data\n",
    "final_data = pd.read_csv('C:/Users/katta/Downloads/formatted_data.csv')\n",
    "\n",
    "# Define constants\n",
    "WINDOW_SIZE = 500  # For 10 seconds of data at 50 Hz\n",
    "OVERLAP = 0  # No overlap\n",
    "\n",
    "# Define activity mapping\n",
    "activity_map = {\n",
    "    'walking': 1,\n",
    "    'standing': 5,\n",
    "    'sitting': 4,\n",
    "    'laying': 6,\n",
    "    'walking_upstairs': 2,\n",
    "    'walking_downstairs': 3\n",
    "}\n",
    "\n",
    "# Map activities to numbers\n",
    "final_data['activity'] = final_data['activity'].map(activity_map)\n",
    "\n",
    "# Function to create windows\n",
    "def create_windows(df, window_size=WINDOW_SIZE, overlap=OVERLAP):\n",
    "    windows = []\n",
    "    labels = []\n",
    "    num_samples = len(df)\n",
    "    start = 0\n",
    "    \n",
    "    while start + window_size <= num_samples:\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        windows.append(window)\n",
    "        labels.append(window['activity'].iloc[0])\n",
    "        start += window_size - overlap\n",
    "        \n",
    "    return windows, labels\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(window):\n",
    "    features = {}\n",
    "    features['mean_acc_x'] = window['acc_x'].mean()\n",
    "    features['mean_acc_y'] = window['acc_y'].mean()\n",
    "    features['mean_acc_z'] = window['acc_z'].mean()\n",
    "    features['std_acc_x'] = window['acc_x'].std()\n",
    "    features['std_acc_y'] = window['acc_y'].std()\n",
    "    features['std_acc_z'] = window['acc_z'].std()\n",
    "    features['mean_total_acc'] = window['total_acc'].mean()\n",
    "    features['std_total_acc'] = window['total_acc'].std()\n",
    "    return features\n",
    "\n",
    "# Create windows and extract features\n",
    "windows, labels = create_windows(final_data)\n",
    "\n",
    "# Extract features for each window\n",
    "features_list = [extract_features(window) for window in windows]\n",
    "features_df = pd.DataFrame(features_list)\n",
    "features_df['activity'] = labels\n",
    "\n",
    "# Save to CSV files in UCI HAR format\n",
    "features_df.to_csv('formatted_features.csv', index=False)\n",
    "print(features_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "0a946edc-204e-462c-8152-44564b538dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.50\n",
      "Precision: 0.42\n",
      "Recall: 0.50\n",
      "Confusion Matrix:\n",
      "[[1 0 0 0 0 0]\n",
      " [0 1 0 0 0 0]\n",
      " [1 0 0 0 0 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0]]\n",
      "Accuracy on collected data: 0.83\n",
      "Precision on collected data: 0.85\n",
      "Recall on collected data: 0.83\n",
      "Confusion Matrix on collected data:\n",
      "[[3 0 0 0 0 0]\n",
      " [0 3 0 0 0 0]\n",
      " [1 0 2 0 0 0]\n",
      " [0 0 0 2 0 1]\n",
      " [0 0 0 0 3 0]\n",
      " [0 0 0 1 0 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Constants\n",
    "time = 10\n",
    "offset = 100\n",
    "folders = [\"LAYING\",\"SITTING\",\"STANDING\",\"WALKING\",\"WALKING_DOWNSTAIRS\",\"WALKING_UPSTAIRS\"]\n",
    "classes = {\"WALKING\":1,\"WALKING_UPSTAIRS\":2,\"WALKING_DOWNSTAIRS\":3,\"SITTING\":4,\"STANDING\":5,\"LAYING\":6}\n",
    "\n",
    "combined_dir = os.path.join(\"Combined\")\n",
    "\n",
    "# Data Preparation for Training and Testing\n",
    "# (Existing code remains unchanged)\n",
    "\n",
    "# concatenate the training and testing data\n",
    "X = np.concatenate((X_train,X_test))\n",
    "y = np.concatenate((y_train,y_test))\n",
    "\n",
    "# split the data into training and testing sets\n",
    "seed = 4\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=seed, stratify=y)\n",
    "\n",
    "# Train Decision Tree model\n",
    "classifier = DecisionTreeClassifier(random_state=42)\n",
    "classifier.fit(X_train.reshape(X_train.shape[0], -1), y_train)  # Flatten for fitting\n",
    "y_pred = classifier.predict(X_test.reshape(X_test.shape[0], -1))\n",
    "\n",
    "# Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Additional Code for Predicting on Collected Data\n",
    "# Load your collected data\n",
    "collected_data_path = 'formatted_features.csv'\n",
    "collected_data = pd.read_csv(collected_data_path)\n",
    "X_collected = collected_data.drop('activity', axis=1).values\n",
    "y_collected = collected_data['activity']\n",
    "\n",
    "# Predict on collected data\n",
    "y_collected_pred = classifier.predict(X_collected)\n",
    "\n",
    "# Evaluate on collected data\n",
    "accuracy_collected = accuracy_score(y_collected, y_collected_pred)\n",
    "precision_collected = precision_score(y_collected, y_collected_pred, average='weighted')\n",
    "recall_collected = recall_score(y_collected, y_collected_pred, average='weighted')\n",
    "conf_matrix_collected = confusion_matrix(y_collected, y_collected_pred)\n",
    "\n",
    "print(f\"Accuracy on collected data: {accuracy_collected:.2f}\")\n",
    "print(f\"Precision on collected data: {precision_collected:.2f}\")\n",
    "print(f\"Recall on collected data: {recall_collected:.2f}\")\n",
    "print(f\"Confusion Matrix on collected data:\\n{conf_matrix_collected}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dbd4df-ee81-406d-9943-bc7a7d84463f",
   "metadata": {},
   "source": [
    "The model was trained on the UCI HAR dataset, which has specific characteristics and patterns related to the sensor data of different activities. When you collected your own data, it was essential that this data was as close as possible to the UCI HAR dataset in terms of the types of features (like accelerometer readings) and their distributions. By doing so, the model could effectively generalize and recognize similar patterns in your collected data.\n",
    "\n",
    "Accuracy on Collected Data: 0.83 (83%)\n",
    "Precision on Collected Data: 0.85 (85%)\n",
    "Recall on Collected Data: 0.83 (83%)\n",
    "\n",
    "The model performed well on your collected data, achieving an accuracy of 83%. The precision and recall values also indicate that the model was effective in correctly identifying the activities in your data. The confusion matrix shows that while there were some misclassifications, the model generally made accurate predictions.\n",
    "\n",
    "This performance suggests that the data you collected was sufficiently similar to the UCI HAR dataset, allowing the trained model to apply its learned patterns effectively. However, the presence of some misclassifications indicates that there might be subtle differences between your data and the UCI HAR dataset, which could be explored further to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d49921-71d3-49c1-8cbb-50d973ccbbc2",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "db46864c-0ed9-417c-9356-265bba85f6fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on collected data: 0.83\n",
      "Precision on collected data: 0.85\n",
      "Recall on collected data: 0.83\n",
      "Confusion Matrix on collected data:\n",
      "[[3 0 0 0 0 0]\n",
      " [0 3 0 0 0 0]\n",
      " [1 0 2 0 0 0]\n",
      " [0 0 0 2 0 1]\n",
      " [0 0 0 0 3 0]\n",
      " [0 0 0 1 0 2]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but DecisionTreeClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_collected_scaled = scaler.fit_transform(X_collected)\n",
    "import numpy as np\n",
    "\n",
    "def extract_fft_features(window):\n",
    "    features = {}\n",
    "    # FFT for each axis\n",
    "    for axis in ['acc_x', 'acc_y', 'acc_z']:\n",
    "        fft_values = np.fft.fft(window[axis].values)\n",
    "        fft_magnitude = np.abs(fft_values)\n",
    "        features[f'fft_mean_{axis}'] = np.mean(fft_magnitude)\n",
    "        features[f'fft_std_{axis}'] = np.std(fft_magnitude)\n",
    "    return features\n",
    "windows, labels = create_windows(final_data)  # Assuming your windowing function is already defined\n",
    "features_list = [extract_features(window) for window in windows]\n",
    "features_df = pd.DataFrame(features_list)\n",
    "y_collected_pred = classifier.predict(features_df)\n",
    "accuracy_collected = accuracy_score(y_collected, y_collected_pred)\n",
    "precision_collected = precision_score(y_collected, y_collected_pred, average='weighted')\n",
    "recall_collected = recall_score(y_collected, y_collected_pred, average='weighted')\n",
    "conf_matrix_collected = confusion_matrix(y_collected, y_collected_pred)\n",
    "\n",
    "print(f\"Accuracy on collected data: {accuracy_collected:.2f}\")\n",
    "print(f\"Precision on collected data: {precision_collected:.2f}\")\n",
    "print(f\"Recall on collected data: {recall_collected:.2f}\")\n",
    "print(f\"Confusion Matrix on collected data:\\n{conf_matrix_collected}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7595ff-feda-41ba-bc1e-badddf499c21",
   "metadata": {},
   "source": [
    "Preprocessing: Scaled the collected data using StandardScaler.\n",
    "Featurization: Applied FFT to extract frequency-domain features for acc_x, acc_y, and acc_z.\n",
    "The Decision Tree model performed well on the collected data with high accuracy, precision, and recall.\n",
    "Preprocessing (scaling) and featurization (FFT) were effectivee in preparing the data for accurate predictions.\n",
    "The model's confusion matrix reveals some areas where it could improve, particularly in distinguishing between certain activities. Fine-tuning the model or using more advanced features could potentially enhance performance further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b87cdf-b31b-408a-b4a9-2daec88ea30a",
   "metadata": {},
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "92c29b87-0701-46bb-a79d-4113395da6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Classification Result: A new example to classify!\n",
      "\n",
      "Let's analyze the statistical features of the new example:\n",
      "\n",
      "* Mean (X): 0.9812, Std Dev (X): 0.1887\n",
      "* Mean (Y): -0.3289, Std Dev (Y): 0.1591\n",
      "* Mean (Z): 0.0787, Std Dev (Z): 0.1218\n",
      "\n",
      "Comparing these features with the examples in the training data, I notice that they are very similar to Example 1, which corresponds to the activity \"WALKING\".\n",
      "\n",
      "In particular, the mean and standard deviation values for X, Y, and Z are all close to the values in Example 1. The differences are small and can be attributed to natural variability in the data.\n",
      "\n",
      "Therefore, based on the similarity of the statistical features, I classify this new example as:\n",
      "\n",
      "**WALKING**\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    {\n",
    "        'mean_x': 0.8,\n",
    "        'std_x': 0.2,\n",
    "        'mean_y': -0.3,\n",
    "        'std_y': 0.15,\n",
    "        'mean_z': 0.1,\n",
    "        'std_z': 0.1,\n",
    "        'activity': 'WALKING'\n",
    "    },\n",
    "    {\n",
    "        'mean_x': 0.1,\n",
    "        'std_x': 0.1,\n",
    "        'mean_y': 0.0,\n",
    "        'std_y': 0.1,\n",
    "        'mean_z': 0.2,\n",
    "        'std_z': 0.2,\n",
    "        'activity': 'SITTING'\n",
    "    },\n",
    "    {\n",
    "        'mean_x': 0.9,\n",
    "        'std_x': 0.25,\n",
    "        'mean_y': -0.2,\n",
    "        'std_y': 0.2,\n",
    "        'mean_z': 0.3,\n",
    "        'std_z': 0.3,\n",
    "        'activity': 'WALKING_UPSTAIRS'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Function to prepare and send a query for few-shot classification\n",
    "def classify_activity_few_shot(features, examples):\n",
    "    # Format the examples into the query\n",
    "    examples_text = \"\\n\".join(\n",
    "        [f\"Example {i+1}:\\nMean (X): {ex['mean_x']}, Std Dev (X): {ex['std_x']}, \"\n",
    "         f\"Mean (Y): {ex['mean_y']}, Std Dev (Y): {ex['std_y']}, \"\n",
    "         f\"Mean (Z): {ex['mean_z']}, Std Dev (Z): {ex['std_z']}\\nActivity: {ex['activity']}\\n\"\n",
    "         for i, ex in enumerate(examples)]\n",
    "    )\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    You are a classifier model trained to identify human activities based on statistical features.\n",
    "    Given the following examples of activities with their statistical features:\n",
    "\n",
    "    {examples_text}\n",
    "\n",
    "    *Classify the following data into one of the above activities:*\n",
    "    - Mean (X): {features['mean_x']}\n",
    "    - Std Dev (X): {features['std_x']}\n",
    "    - Mean (Y): {features['mean_y']}\n",
    "    - Std Dev (Y): {features['std_y']}\n",
    "    - Mean (Z): {features['mean_z']}\n",
    "    - Std Dev (Z): {features['std_z']}\n",
    "    \"\"\"\n",
    "\n",
    "    # Send the query to the model\n",
    "    response = llm.invoke(query)\n",
    "    # The response content should be the activity name\n",
    "    return response.content.strip()\n",
    "\n",
    "# Example features for classification\n",
    "features = {\n",
    "    'mean_x': 0.9812,\n",
    "    'std_x': 0.1887,\n",
    "    'mean_y': -0.3289,\n",
    "    'std_y': 0.1591,\n",
    "    'mean_z': 0.0787,\n",
    "    'std_z': 0.1218\n",
    "}\n",
    "\n",
    "# Perform classification\n",
    "classification_result = classify_activity_few_shot(features, examples)\n",
    "print(f\"Few-Shot Classification Result: {classification_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "45ea8ab9-330c-460e-b993-d62ad129c0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Few-Shot Classification Result: Based on the provided statistical features, I would classify the given data as **WALKING**.\n",
      "\n",
      "Here's a brief explanation of my reasoning:\n",
      "\n",
      "* The mean and standard deviation of X (0.9728 and 0.01495) are closest to the values in Example 1 (WALKING), which are 0.8 and 0.2, respectively.\n",
      "* The mean and standard deviation of Y (-0.3147 and 0.01082) are also similar to the values in Example 1 (WALKING), which are -0.3 and 0.15, respectively.\n",
      "* The mean and standard deviation of Z (0.0759 and 0.00388) are relatively close to the values in Example 1 (WALKING), which are 0.1 and 0.1, respectively.\n",
      "\n",
      "While there are some differences between the given data and Example 1, the overall pattern and magnitudes of the statistical features suggest that the activity is most likely **WALKING**.\n",
      "True Label: WALKING_UPSTAIRS\n",
      "Predicted Label: Based on the provided statistical features, I would classify the given data as **WALKING**.\n",
      "\n",
      "Here's a brief explanation of my reasoning:\n",
      "\n",
      "* The mean and standard deviation of X (0.9728 and 0.01495) are closest to the values in Example 1 (WALKING), which are 0.8 and 0.2, respectively.\n",
      "* The mean and standard deviation of Y (-0.3147 and 0.01082) are also similar to the values in Example 1 (WALKING), which are -0.3 and 0.15, respectively.\n",
      "* The mean and standard deviation of Z (0.0759 and 0.00388) are relatively close to the values in Example 1 (WALKING), which are 0.1 and 0.1, respectively.\n",
      "\n",
      "While there are some differences between the given data and Example 1, the overall pattern and magnitudes of the statistical features suggest that the activity is most likely **WALKING**.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example features for activities, ideally these should be extracted from UCI-HAR dataset\n",
    "examples = [\n",
    "    {\n",
    "        'mean_x': 0.8,\n",
    "        'std_x': 0.2,\n",
    "        'mean_y': -0.3,\n",
    "        'std_y': 0.15,\n",
    "        'mean_z': 0.1,\n",
    "        'std_z': 0.1,\n",
    "        'activity': 'WALKING'\n",
    "    },\n",
    "    {\n",
    "        'mean_x': 0.1,\n",
    "        'std_x': 0.1,\n",
    "        'mean_y': 0.0,\n",
    "        'std_y': 0.1,\n",
    "        'mean_z': 0.2,\n",
    "        'std_z': 0.2,\n",
    "        'activity': 'SITTING'\n",
    "    },\n",
    "    {\n",
    "        'mean_x': 0.9,\n",
    "        'std_x': 0.25,\n",
    "        'mean_y': -0.2,\n",
    "        'std_y': 0.2,\n",
    "        'mean_z': 0.3,\n",
    "        'std_z': 0.3,\n",
    "        'activity': 'WALKING_UPSTAIRS'\n",
    "    }\n",
    "]\n",
    "\n",
    "def preprocess_features(data):\n",
    "    # Example preprocessing: mean and standard deviation calculations\n",
    "    mean_x = np.mean(data['x'])\n",
    "    std_x = np.std(data['x'])\n",
    "    mean_y = np.mean(data['y'])\n",
    "    std_y = np.std(data['y'])\n",
    "    mean_z = np.mean(data['z'])\n",
    "    std_z = np.std(data['z'])\n",
    "    return {\n",
    "        'mean_x': mean_x,\n",
    "        'std_x': std_x,\n",
    "        'mean_y': mean_y,\n",
    "        'std_y': std_y,\n",
    "        'mean_z': mean_z,\n",
    "        'std_z': std_z\n",
    "    }\n",
    "\n",
    "def classify_activity_few_shot(features, examples):\n",
    "    # Format the examples into the query\n",
    "    examples_text = \"\\n\".join(\n",
    "        [f\"Example {i+1}:\\nMean (X): {ex['mean_x']}, Std Dev (X): {ex['std_x']}, \"\n",
    "         f\"Mean (Y): {ex['mean_y']}, Std Dev (Y): {ex['std_y']}, \"\n",
    "         f\"Mean (Z): {ex['mean_z']}, Std Dev (Z): {ex['std_z']}\\nActivity: {ex['activity']}\\n\"\n",
    "         for i, ex in enumerate(examples)]\n",
    "    )\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    You are a classifier model trained to identify human activities based on statistical features.\n",
    "    Given the following examples of activities with their statistical features:\n",
    "\n",
    "    {examples_text}\n",
    "\n",
    "    *Classify the following data into one of the above activities:*\n",
    "    - Mean (X): {features['mean_x']}\n",
    "    - Std Dev (X): {features['std_x']}\n",
    "    - Mean (Y): {features['mean_y']}\n",
    "    - Std Dev (Y): {features['std_y']}\n",
    "    - Mean (Z): {features['mean_z']}\n",
    "    - Std Dev (Z): {features['std_z']}\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace this with actual model invocation code\n",
    "    response = llm.invoke(query)  # You need to define how to invoke the model\n",
    "    return response.content.strip()\n",
    "\n",
    "# Example data from UCI-HAR dataset for testing\n",
    "test_data = {\n",
    "    'x': [0.9812, 0.95, 0.97, 0.99],  # Sample x values\n",
    "    'y': [-0.3289, -0.32, -0.30, -0.31],  # Sample y values\n",
    "    'z': [0.0787, 0.08, 0.07, 0.075]  # Sample z values\n",
    "}\n",
    "\n",
    "# Preprocess the test data\n",
    "processed_features = preprocess_features(test_data)\n",
    "\n",
    "# Perform classification\n",
    "classification_result = classify_activity_few_shot(processed_features, examples)\n",
    "print(f\"Few-Shot Classification Result: {classification_result}\")\n",
    "\n",
    "# Evaluation logic (replace with actual labels and evaluation metrics)\n",
    "true_label = 'WALKING_UPSTAIRS'  # Replace with actual true label for evaluation\n",
    "print(f\"True Label: {true_label}\")\n",
    "print(f\"Predicted Label: {classification_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9255c5-5637-4872-af93-43c8ee216e3e",
   "metadata": {},
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "cdcb9ccc-2ce7-4a61-9959-60dcd6f93c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on collected data: 0.00\n",
      "Precision on collected data: 0.00\n",
      "Recall on collected data: 0.00\n",
      "Confusion Matrix on collected data:\n",
      "[[0 0 0 0 0 0 0]\n",
      " [3 0 0 0 0 0 0]\n",
      " [3 0 0 0 0 0 0]\n",
      " [3 0 0 0 0 0 0]\n",
      " [3 0 0 0 0 0 0]\n",
      " [3 0 0 0 0 0 0]\n",
      " [3 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Retrieve the API key from environment variables\n",
    "Groq_Token = os.getenv('GROQ_API_KEY')\n",
    "\n",
    "# Define the API endpoint (adjust based on actual endpoint)\n",
    "api_endpoint = 'https://api.groq.com/v1/predict'  # Replace with the actual endpoint\n",
    "\n",
    "# Define few-shot examples\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        'mean_x': 0.8, 'std_x': 0.2, 'mean_y': -0.3, 'std_y': 0.15, 'mean_z': 0.1, 'std_z': 0.1, 'activity': 'WALKING'\n",
    "    },\n",
    "    {\n",
    "        'mean_x': 0.1, 'std_x': 0.1, 'mean_y': 0.0, 'std_y': 0.1, 'mean_z': 0.2, 'std_z': 0.2, 'activity': 'SITTING'\n",
    "    },\n",
    "    {\n",
    "        'mean_x': 0.9, 'std_x': 0.25, 'mean_y': -0.2, 'std_y': 0.2, 'mean_z': 0.3, 'std_z': 0.3, 'activity': 'WALKING_UPSTAIRS'\n",
    "    }\n",
    "]\n",
    "\n",
    "def preprocess_features(data):\n",
    "    mean_x = np.mean(data['x'])\n",
    "    std_x = np.std(data['x'])\n",
    "    mean_y = np.mean(data['y'])\n",
    "    std_y = np.std(data['y'])\n",
    "    mean_z = np.mean(data['z'])\n",
    "    std_z = np.std(data['z'])\n",
    "    return {\n",
    "        'mean_x': mean_x,\n",
    "        'std_x': std_x,\n",
    "        'mean_y': mean_y,\n",
    "        'std_y': std_y,\n",
    "        'mean_z': mean_z,\n",
    "        'std_z': std_z\n",
    "    }\n",
    "\n",
    "def classify_activity_few_shot(features, examples):\n",
    "    examples_text = \"\\n\".join(\n",
    "        [f\"Example {i+1}:\\nMean (X): {ex['mean_x']}, Std Dev (X): {ex['std_x']}, \"\n",
    "         f\"Mean (Y): {ex['mean_y']}, Std Dev (Y): {ex['std_y']}, \"\n",
    "         f\"Mean (Z): {ex['mean_z']}, Std Dev (Z): {ex['std_z']}\\nActivity: {ex['activity']}\\n\"\n",
    "         for i, ex in enumerate(examples)]\n",
    "    )\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    You are a classifier model trained to identify human activities based on statistical features.\n",
    "    Given the following examples of activities with their statistical features:\n",
    "\n",
    "    {examples_text}\n",
    "\n",
    "    *Classify the following data into one of the above activities:*\n",
    "    - Mean (X): {features['mean_x']}\n",
    "    - Std Dev (X): {features['std_x']}\n",
    "    - Mean (Y): {features['mean_y']}\n",
    "    - Std Dev (Y): {features['std_y']}\n",
    "    - Mean (Z): {features['mean_z']}\n",
    "    - Std Dev (Z): {features['std_z']}\n",
    "    \"\"\"\n",
    "\n",
    "    # Send request to the Groq API\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {Groq_Token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        'prompt': query,\n",
    "        'examples': examples  # Include few-shot examples in the request\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(api_endpoint, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        response_data = response.json()\n",
    "        \n",
    "        # Print response data for debugging\n",
    "        print(\"Response Data:\", response_data)\n",
    "        \n",
    "        # Adjust based on actual response format\n",
    "        if 'predictions' in response_data:\n",
    "            return response_data['predictions'][0]  # Adjust based on actual response\n",
    "        else:\n",
    "            return 'Predictions key not found in response'\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Request failed: {e}\"\n",
    "\n",
    "\n",
    "# Load and preprocess collected data\n",
    "X_collected = pd.read_csv('formatted_features.csv')\n",
    "\n",
    "# Define true labels for evaluation\n",
    "true_labels = X_collected['activity'].tolist()\n",
    "\n",
    "# Extract features for prediction\n",
    "X_collected_features = X_collected[['mean_acc_x', 'mean_acc_y', 'mean_acc_z', 'std_acc_x', 'std_acc_y', 'std_acc_z', 'mean_total_acc', 'std_total_acc']].to_dict(orient='records')\n",
    "\n",
    "# Map string labels to numeric values for evaluation\n",
    "all_labels = set(true_labels) | set()  # Add your predicted labels here as well\n",
    "label_to_numeric = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
    "\n",
    "# Convert true labels to numeric\n",
    "true_labels_numeric = [label_to_numeric[label] for label in true_labels]\n",
    "\n",
    "# Perform classification for each feature set\n",
    "predicted_labels = []\n",
    "for features in X_collected_features:\n",
    "    processed_features = preprocess_features({\n",
    "        'x': [features['mean_acc_x']],\n",
    "        'y': [features['mean_acc_y']],\n",
    "        'z': [features['mean_acc_z']]\n",
    "    })\n",
    "    # Pass both processed_features and few_shot_examples\n",
    "    classification_result = classify_activity_few_shot(processed_features, few_shot_examples)\n",
    "    predicted_labels.append(classification_result)\n",
    "\n",
    "\n",
    "# Convert predicted labels to numeric\n",
    "predicted_labels_numeric = [label_to_numeric.get(label, -1) for label in predicted_labels]\n",
    "\n",
    "# Evaluate performance\n",
    "if len(true_labels_numeric) == len(predicted_labels_numeric):\n",
    "    accuracy_collected = accuracy_score(true_labels_numeric, predicted_labels_numeric)\n",
    "    precision_collected = precision_score(true_labels_numeric, predicted_labels_numeric, average='weighted')\n",
    "    recall_collected = recall_score(true_labels_numeric, predicted_labels_numeric, average='weighted')\n",
    "    conf_matrix_collected = confusion_matrix(true_labels_numeric, predicted_labels_numeric)\n",
    "\n",
    "    print(f\"Accuracy on collected data: {accuracy_collected:.2f}\")\n",
    "    print(f\"Precision on collected data: {precision_collected:.2f}\")\n",
    "    print(f\"Recall on collected data: {recall_collected:.2f}\")\n",
    "    print(f\"Confusion Matrix on collected data:\\n{conf_matrix_collected}\")\n",
    "else:\n",
    "    print(\"Length mismatch between true labels and predicted labels.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada7104d-dfa9-487b-ad50-b7317c00b4e2",
   "metadata": {},
   "source": [
    "\"The model's performance on the collected data shows 0.00 accuracy, precision, and recall, with a confusion matrix indicating no correct predictions across any classes. This outcome suggests potential issues with API integration, feature mismatch, few-shot prompting misconfiguration, or errors in data collection. Given that we encountered 404 errors previously, it’s likely that the API endpoint or configuration needs to be reviewed and corrected. Additionally, verifying feature preprocessing and ensuring that the few-shot examples are representative and correctly formatted will be crucial in improving model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27561b7e-552e-4c30-8517-be2598300fae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
